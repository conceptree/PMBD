{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# United States Weather Forecast with Spark\n",
    "\n",
    "Projeto de mestrado de Ciência de dados do ISCTE - Instituto Universitário de Lisboa para a cadeira de Processamento, Modelação de Big Data.\n",
    "\n",
    "## Authors\n",
    "\n",
    "- João Silva\n",
    "- Nuno Rodrigues\n",
    "- Tiago Alves\n",
    "\n",
    "## Datasets \n",
    "\n",
    "US weather history : https://www.kaggle.com/datasets/nachiketkamod/weather-dataset-us\n",
    "US Noa Stations : assets/ghcnd-stations.txt\n",
    "\n",
    "# Transformação/Modelação de dados\n",
    "\n",
    "## Passo 1\n",
    "\n",
    "Neste primeiro passo vamos converter o ficheiro que lista todas as estações meteorológicas e respetivos nomes das suas localizações.\n",
    "\n",
    "Para isto vamos então usar o ghcnd-stations.txt  e converter para um .csv válido que possamos usar para cruzamento dos ids das estações listadas no dataset principal e as suas localizações.\n",
    "\n",
    "Para isto vamos usar a biblioteca \"csv\" do python e declarar as variáveis para os caminhos do ficheiro de entrada e de saída."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32f44314242d9525"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Nome do arquivo de entrada e saída\n",
    "input_file = './assets/ghcnd-stations.txt'\n",
    "output_file = './assets/ghcnd_stations.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6566193494f355ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos agora correr a lógica responsável por abrir o ficheiro de saída, definir os cabeçalhos, abrir o ficheiro de entrada para então mapear linha a linha de forma a finalmente escrever o ficheiro final."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fca978926a1f60b9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Abrir arquivo de saída\n",
    "with open(output_file, mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Escrever cabeçalho\n",
    "    header = ['station_id', 'latitude', 'longitude', 'elevation', 'state', 'name', 'gsn_flag', 'hcn_crn_flag', 'wmo_id']\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Ler arquivo de entrada e processar linha por linha\n",
    "    with open(input_file, mode='r') as txt_file:\n",
    "        for line in txt_file:\n",
    "            # Extrair dados baseado nas posições fixas das colunas\n",
    "            station_id = line[0:11].strip()\n",
    "            latitude = line[12:20].strip()\n",
    "            longitude = line[21:30].strip()\n",
    "            elevation = line[31:37].strip()\n",
    "            state = line[38:40].strip()\n",
    "            name = line[41:71].strip()\n",
    "            gsn_flag = line[72:75].strip()\n",
    "            hcn_crn_flag = line[76:79].strip()\n",
    "            wmo_id = line[80:85].strip()\n",
    "\n",
    "            # Escrever linha no arquivo CSV\n",
    "            csv_writer.writerow([station_id, latitude, longitude, elevation, state, name, gsn_flag, hcn_crn_flag, wmo_id])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d822c22c571a69d1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Arquivo CSV '{output_file}' criado com sucesso.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3bceea4c6bfa98a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 2 (Opcional)\n",
    "\n",
    "Este passo é apenas necessário caso já exista uma versão de spark associada a outra versão de python diferente da qual se encontra associada a este projeto. Caso se encontre nesta situação deve então definir as variáveis de ambiente que apontaram para a versão correta.\n",
    "\n",
    "Aqui fica o exemplo:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4619b436cb46727"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Definir a mesma versão do Python para driver e worker\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python3.11'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/local/bin/python3.11'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a8046a56b12735e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 3\n",
    "\n",
    "Vamos então agora iniciar a sessão de spark e importar as bibliotecas necessárias para os passos seguintes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15a15379cbf5392b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, round, floor\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Weather Dataset with Location\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac019203f2130fc5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 4\n",
    "\n",
    "Vamos ler o dataset alvo deste projeto. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb0dd8358e922c87"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "file_path = \"./assets/sample_dataset.csv\"\n",
    "weather_df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b1033c99b20ef7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 5\n",
    "\n",
    "Vamos ler o dataset que lista as estações e as suas localizações préviamente convertido de txt para csv como referido nos passos em cima."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcf6b86020c8c605"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "stations_file_path = \"./assets/ghcnd_stations.csv\"\n",
    "stations_df = spark.read.csv(stations_file_path, header=True, inferSchema=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e45fbc6bc1b8a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 6\n",
    "\n",
    "De forma a evitar conflito entre as colunas de ambos os datasets que iremos cruzar, vamos então renomear algumas das colunas do dataset das estações cujo nome se encontra igual ao dataset principal."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5606613fa793556"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "stations_df = stations_df.withColumnRenamed(\"elevation\", \"station_elevation\")  \\\n",
    "                        .withColumnRenamed(\"latitude\", \"station_latitude\") \\\n",
    "                        .withColumnRenamed(\"name\", \"Location\") \\\n",
    "                        .withColumnRenamed(\"longitude\", \"station_longitude\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7750e80218e8d49b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 7\n",
    "\n",
    "Neste passo vamos combinar ambos os datasets de forma a finar com apenas uma única fonte de verdade para as tranformações seguintes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b68c5a690d4ad33"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weather_df_with_location = weather_df.join(stations_df, weather_df.ID == stations_df.station_id, how='left')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d965de44c2323d57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 8\n",
    "\n",
    "Como primeira transformação iremos converter as colunas referentes às temperaturas máximas e minimas para graus Celcius. Tendo em conta que os dados originais parecem encontrar-se multiplicados por 10, iremos inverter o racional dividindo por 10. \n",
    "\n",
    "Vamos também lidar com alguns possiveis outliers recorrendo ao método IQR e Z-Score.\n",
    "\n",
    "Referência: https://www.machinelearningplus.com/pyspark/pyspark-outlier-detection-and-treatment/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb39be25eb094085"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aqui iremos também aproveitar para renomear as colunas \"TMAX\" e \"TMIN\" para \"temp_max\" e \"temp_min\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1f63542a755beef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weather_df_with_location = weather_df_with_location \\\n",
    "    .withColumn(\"temp_max\", col(\"TMAX\") / 10) \\\n",
    "    .withColumn(\"temp_min\", col(\"TMIN\") / 10)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b7648af2b711c5d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Função IQR."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a05a38cdff0c74cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calcular_limites_iqr(df, coluna):\n",
    "    quantiles = df.approxQuantile(coluna, [0.25, 0.75], 0.0)\n",
    "    q1, q3 = quantiles\n",
    "    iqr = q3 - q1\n",
    "    limite_inferior = q1 - 1.5 * iqr\n",
    "    limite_superior = q3 + 1.5 * iqr\n",
    "    return limite_inferior, limite_superior\n",
    "\n",
    "temp_max_inferior, temp_max_superior = calcular_limites_iqr(weather_df_with_location, \"temp_max\")\n",
    "temp_min_inferior, temp_min_superior = calcular_limites_iqr(weather_df_with_location, \"temp_min\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94aa76d3c6865d42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filtrar entradas com base nos resultados."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ec928b25989dc17"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weather_df_with_location = weather_df_with_location.filter(\n",
    "    (col(\"temp_max\") >= temp_max_inferior) & (col(\"temp_max\") <= temp_max_superior) &\n",
    "    (col(\"temp_min\") >= temp_min_inferior) & (col(\"temp_min\") <= temp_min_superior)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d36a5bb79027c496"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Função z-score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d472bd6a73c191ac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Método Z-score para detectar outliers\n",
    "def calcular_limites_z_score(df, coluna):\n",
    "    stats = df.select(_mean(col(coluna)).alias('mean'), _stddev(col(coluna)).alias('stddev')).collect()\n",
    "    mean = stats[0]['mean']\n",
    "    stddev = stats[0]['stddev']\n",
    "    limite_inferior = mean - 3 * stddev\n",
    "    limite_superior = mean + 3 * stddev\n",
    "    return limite_inferior, limite_superior\n",
    "\n",
    "temp_max_inferior, temp_max_superior = calcular_limites_z_score(weather_df_with_location, \"temp_max\")\n",
    "temp_min_inferior, temp_min_superior = calcular_limites_z_score(weather_df_with_location, \"temp_min\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3dc18d8c653fd53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filtrar entradas com base nos resultados."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b340cde1157c0ed"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weather_df_with_location = weather_df_with_location.filter(\n",
    "    (col(\"temp_max\") >= temp_max_inferior) & (col(\"temp_max\") <= temp_max_superior) &\n",
    "    (col(\"temp_min\") >= temp_min_inferior) & (col(\"temp_min\") <= temp_min_superior)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17b7ce3fb72f022f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 9\n",
    "\n",
    "Iremos agora arredondar os valores da coluna \"Elevation\" de forma inteiros mais legiveis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a420e04b349512f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weather_df_with_location = weather_df_with_location.withColumn(\"Elevation\", floor(round(col(\"Elevation\"))))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ebbc521f275948b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 10\n",
    "\n",
    "Vamos renomear as colunas EVAP e PRCP para \"Evaporation\" e \"Precipitation\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba544631ac55e323"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weather_df_with_location = weather_df_with_location.withColumnRenamed(\"EVAP\", \"Evaporation\") \\\n",
    "                                                   .withColumnRenamed(\"PRCP\", \"Precipitation\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15f020e4a971c71a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 11\n",
    "\n",
    "Vamos filtrar os nossos dados por todas as entradas que não tenham as colunas \"Precipitation\" ou \"temp_max\" ou \"temp_min\" que não sejam nulas, ou seja, basta uma delas ter um valor nulo para ser descartada da nossa análise final."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcfa31deeb99bf34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weather_df_with_location = weather_df_with_location.filter(\n",
    "    col(\"Precipitation\").isNotNull() &\n",
    "    col(\"temp_max\").isNotNull() &\n",
    "    col(\"temp_min\").isNotNull()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f30332fdb95a496c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 12\n",
    "\n",
    "Neste passo vamos definir quais as colunas que queremos considerar para o nosso dataset final."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb3594321130989a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weather_df_with_location = weather_df_with_location.select(\n",
    "    \"ID\", \"DATE\", \"temp_max\", \"temp_min\", \"Precipitation\",\n",
    "    \"Latitude\", \"Longitude\", \"Elevation\", \"Location\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45d9f76517105a30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo 13\n",
    "\n",
    "Vamos então agora escrever o ficheiro final."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64742ab2900c2aac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output_path = \"./assets/sample_weather_dataset_with_location.csv\"\n",
    "weather_df_with_location.coalesce(1).write.csv(output_path, header=True, mode='overwrite')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48565f371a266027"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Passo final\n",
    "\n",
    "Como passo final iremos imprimir, meramente de forma informativa, que todo o processo de transformação foi concluído com sucesso e gravado na pasta pré-definida, e dar a nossa sessão spark como encerrada. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33054d49a6c901f5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Dataset final salvo em {output_path}\")\n",
    "\n",
    "# Encerrar a Spark Session\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c424d17bb4b5d26d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "_____________________________"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a9fb1aa592fb519"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc8c498a455d6451"
  },
  {
   "cell_type": "markdown",
   "source": [
    "_____________________________"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3f75bf3ce147b4a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbb55170695f1718"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Weather Dataset with Random Forest\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f91b314a91b2d91e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "file_path = \"./assets/sample_dataset.csv\"\n",
    "weather_df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f06a9849f471cbdd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Verificar as colunas\n",
    "weather_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "349d84a68224150a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Persistir em disco\n",
    "weather_df.persist(StorageLevel.DISK_ONLY)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d389fed7fb310f22"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Preparar os dados para o modelo\n",
    "feature_columns = [\"Latitude\", \"Longitude\", \"Elevation\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa64e881e982036f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Transformar o DataFrame\n",
    "weather_df = assembler.transform(weather_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f44a02ac81e52f7f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Dividir o conjunto de dados em treino e teste\n",
    "train_df, test_df = weather_df.randomSplit([0.8, 0.2], seed=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2fa519edc34140c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Treinar e avaliar o modelo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8a951c918b5e2d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def treinar_e_avaliar_random_forest(label_col, train_df, test_df):\n",
    "    # Treinar o modelo de Random Forest\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=label_col, numTrees=100)\n",
    "    rf_model = rf.fit(train_df)\n",
    "\n",
    "    # Fazer previsões no conjunto de teste\n",
    "    predictions = rf_model.transform(test_df)\n",
    "\n",
    "    # Avaliar o modelo\n",
    "    evaluator = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"Root Mean Squared Error (RMSE) no conjunto de teste para {label_col}: {rmse}\")\n",
    "\n",
    "    # Converter para Pandas para visualização\n",
    "    predictions_pd = predictions.select(\"prediction\", label_col).toPandas()\n",
    "\n",
    "    # Gerar gráfico de dispersão\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=label_col, y=\"prediction\", data=predictions_pd)\n",
    "    plt.xlabel(f\"Actual {label_col}\")\n",
    "    plt.ylabel(f\"Predicted {label_col}\")\n",
    "    plt.title(f\"Actual vs Predicted {label_col}\")\n",
    "    plt.show()\n",
    "\n",
    "    return rf_model, predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58f2800294a6b83e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Iniciar Treino"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fcac3defc80b0df"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# temp_max\n",
    "rf_model_temp_max, predictions_temp_max = treinar_e_avaliar_random_forest(\"temp_max\", train_df, test_df)\n",
    "\n",
    "# temp_min\n",
    "rf_model_temp_min, predictions_temp_min = treinar_e_avaliar_random_forest(\"temp_min\", train_df, test_df)\n",
    "\n",
    "# Precipitation\n",
    "rf_model_precipitation, predictions_precipitation = treinar_e_avaliar_random_forest(\"Precipitation\", train_df, test_df)\n",
    "\n",
    "# Liberar recursos de memória\n",
    "weather_df.unpersist()\n",
    "\n",
    "# Encerrar a Spark Session\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "837b2e819e68c72e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
